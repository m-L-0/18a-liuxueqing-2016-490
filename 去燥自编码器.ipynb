{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ecba2b70177d>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)\n",
    " \n",
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noise_factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-53ebdc49994c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mnoisy_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnoise_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mnoisy_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoisy_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'noise_factor' is not defined"
     ]
    }
   ],
   "source": [
    "#卷积去噪自编码器\n",
    "\n",
    "### 编码器\n",
    "conv1 = tf.layers.conv2d(inputs_, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 28x28x32\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# 当前shape: 14x14x32\n",
    "conv2 = tf.layers.conv2d(maxpool1, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 14x14x32\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "# 当前shape: 7x7x32\n",
    "conv3 = tf.layers.conv2d(maxpool2, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 7x7x16\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# 当前shape: 4x4x16\n",
    " \n",
    " \n",
    "### 解码器\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
    "# 当前shape: 7x7x16\n",
    "conv4 = tf.layers.conv2d(upsample1, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 7x7x16\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
    "# 当前shape: 14x14x16\n",
    "conv5 = tf.layers.conv2d(upsample2, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 14x14x32\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
    "# 当前shape: 28x28x32\n",
    "conv6 = tf.layers.conv2d(upsample3, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 当前shape: 28x28x32\n",
    " \n",
    " \n",
    "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#当前shape: 28x28x1\n",
    " \n",
    " \n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    " \n",
    " \n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    " \n",
    " \n",
    "sess = tf.Session()\n",
    "epochs = 100\n",
    "batch_size = 20\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(mnist.train.num_examples // batch_size):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        # \n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        # \n",
    "        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n",
    "        # \n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        # \n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,\n",
    "                                                         targets_: imgs})\n",
    "        print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))\n",
    " \n",
    " \n",
    "in_imgs = mnist.test.images[:10]\n",
    "noisy_imgs = in_imgs + noise_factor * np.random.randn(*in_imgs.shape)\n",
    "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: noisy_imgs.reshape((10, 28, 28, 1))})\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(10):\n",
    "    #画原始图像\n",
    "    ax=plt.subplot(2,10,i+1)\n",
    "    plt.imshow(in_imgs[i].reshape((28,28)))\n",
    "    plt.gray()#只有黑白两色，没有中间色\n",
    "    #重建图像\n",
    "    ax=plt.subplot(2,10,i+10+1)\n",
    "    plt.imshow(reconstructed[i].reshape((28,28)))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False) #不显示x轴\n",
    "    ax.get_yaxis().set_visible(False)#不显示y轴\n",
    "plt.show()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)\n",
    " \n",
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现标准的均匀分布的Xaiver初始化器，其中fan_in是输入节点的数量，fan_out是输出节点的数量\n",
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out),\n",
    "        minval = low, maxval = high, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个去噪自编码的类，方便使用。\n",
    "\n",
    "class AdditiveGaussianNoiseAutoencoder(object):\n",
    "    def __init__(self, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer(), scale = 0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.transfer = transfer_function\n",
    "        self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#卷积神经网络\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets( 'MNIST_data', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 2.4244, acc 0.0518\n",
      "step   500, loss 0.5025, acc 0.8786\n",
      "step  1000, loss 0.1703, acc 0.9270\n",
      "step  1500, loss 0.1255, acc 0.9440\n",
      "step  2000, loss 0.1135, acc 0.9533\n",
      "step  2500, loss 0.1612, acc 0.9539\n",
      "step  3000, loss 0.0938, acc 0.9619\n",
      "step  3500, loss 0.2240, acc 0.9645\n",
      "step  4000, loss 0.0918, acc 0.9648\n",
      "step  4500, loss 0.1278, acc 0.9670\n",
      "step  5000, loss 0.0393, acc 0.9714\n",
      "step  5500, loss 0.0999, acc 0.9724\n",
      "step  6000, loss 0.0567, acc 0.9707\n",
      "step  6500, loss 0.0091, acc 0.9732\n",
      "step  7000, loss 0.0499, acc 0.9733\n",
      "step  7500, loss 0.0205, acc 0.9735\n",
      "step  8000, loss 0.0637, acc 0.9733\n",
      "step  8500, loss 0.0684, acc 0.9741\n",
      "step  9000, loss 0.0899, acc 0.9742\n",
      "step  9500, loss 0.1292, acc 0.9763\n",
      "step 10000, loss 0.0112, acc 0.9788\n",
      "step 10500, loss 0.0375, acc 0.9770\n",
      "step 11000, loss 0.1680, acc 0.9765\n",
      "step 11500, loss 0.0490, acc 0.9761\n",
      "step 12000, loss 0.1508, acc 0.9784\n",
      "step 12500, loss 0.0694, acc 0.9765\n",
      "step 13000, loss 0.1314, acc 0.9801\n",
      "step 13500, loss 0.0401, acc 0.9801\n",
      "step 14000, loss 0.0147, acc 0.9804\n",
      "step 14500, loss 0.0341, acc 0.9818\n",
      "step 15000, loss 0.0188, acc 0.9800\n",
      "step 15500, loss 0.0506, acc 0.9812\n",
      "step 16000, loss 0.1986, acc 0.9814\n",
      "step 16500, loss 0.0114, acc 0.9811\n",
      "step 17000, loss 0.1555, acc 0.9806\n",
      "step 17500, loss 0.0051, acc 0.9807\n",
      "step 18000, loss 0.1165, acc 0.9814\n",
      "step 18500, loss 0.0717, acc 0.9816\n",
      "step 19000, loss 0.0601, acc 0.9812\n",
      "step 19500, loss 0.1449, acc 0.9841\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    conv1_weight = tf.get_variable('c1_w', [5, 5, 1, 8], dtype=tf.float32) #权重\n",
    "    conv1_bias = tf.get_variable('c1_b', [8], dtype=tf.float32)  #偏置量\n",
    "    conv1_ = tf.nn.conv2d(inputs, conv1_weight, [1, 1, 1, 1], 'VALID')\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1_ + conv1_bias)\n",
    "    \n",
    "    # shape=[None, 12, 12, 8]\n",
    "    pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    conv2_weight = tf.get_variable('c2_w', shape=[5, 5, 8, 16], dtype=tf.float32)\n",
    "    conv2_bias = tf.get_variable( 'c2_b', shape=[16], dtype=tf.float32)\n",
    "    conv2_ = tf.nn.conv2d(pool1, conv2_weight, [1, 1, 1, 1], 'VALID')\n",
    "    # shape=[None, 8, 8, 16]\n",
    "    conv2 = tf.nn.relu(conv2_ + conv2_bias)\n",
    "    \n",
    "    # shape=[None, 4, 4, 16]\n",
    "    pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    tmp = tf.reshape(pool2, [-1, 4 * 4 * 16])\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(10, activation=None)(tmp)\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
